# NanoGPT Project Structure

This directory contains a research-grade implementation of a Decoder-Only Transformer (GPT) trained on Tiny Shakespeare.

## üìÅ Directory Layout

```
.
‚îú‚îÄ‚îÄ gpt.py                          # Full Transformer implementation with detailed "why" comments
‚îú‚îÄ‚îÄ bigram.py                       # Baseline bigram model for comparison
‚îú‚îÄ‚îÄ train.py                        # CPU-optimized training script (2-3 hours, 3.2M params)
‚îú‚îÄ‚îÄ train_colab.py                  # GPU-ready standalone Python script for Colab/Kaggle
‚îÇ
‚îú‚îÄ‚îÄ colab_nanoGPT.ipynb            # Jupyter notebook for Google Colab (7-section setup)
‚îú‚îÄ‚îÄ myowngpt.ipynb                  # Original exploratory notebook
‚îÇ
‚îú‚îÄ‚îÄ input.txt                       # Tiny Shakespeare dataset (1.1MB, 4.4M characters)
‚îú‚îÄ‚îÄ generated_output.txt            # Output from local CPU training
‚îú‚îÄ‚îÄ more.txt                        # Alternate output (commented in code)
‚îÇ
‚îú‚îÄ‚îÄ README.md                       # Main documentation with architecture details + all 3 samples
‚îú‚îÄ‚îÄ TRAINING_REPORT.md              # Comprehensive training report (this file)
‚îú‚îÄ‚îÄ requirements.txt                # Python dependencies (torch, numpy, matplotlib, jupyter)
‚îÇ
‚îú‚îÄ‚îÄ collab train/                   # Google Colab GPU training artifacts
‚îÇ   ‚îú‚îÄ‚îÄ sample.txt                  # Generated text from Colab run
‚îÇ   ‚îî‚îÄ‚îÄ colab_nanoGPT.ipynb        # Notebook from Colab session
‚îÇ
‚îú‚îÄ‚îÄ kaggle train/                   # Kaggle GPU training artifacts
‚îÇ   ‚îú‚îÄ‚îÄ kagglesampletxt.txt        # Generated text from Kaggle run
‚îÇ   ‚îî‚îÄ‚îÄ myowngptkaggle-ipynb.ipynb # Notebook from Kaggle session
‚îÇ
‚îî‚îÄ‚îÄ .git/                           # Version control
```

## üöÄ Quick Start

### Option 1: CPU Training (Local, 2-3 hours)

```bash
python -m venv .venv
.venv\Scripts\activate              # Windows
python -m pip install -r requirements.txt
python train.py
```

### Option 2: GPU Training (Colab/Kaggle, 30 minutes)

1. Open [Google Colab](https://colab.research.google.com) or [Kaggle](https://kaggle.com)
2. Upload or create a new notebook
3. Copy cells from `colab_nanoGPT.ipynb`
4. Run all cells (GPU enabled: Runtime > Change runtime type > GPU)

Or use the standalone script:

```bash
python train_colab.py  # On Colab/Kaggle with GPU
```

## üìä Model Specifications

| Aspect              | Value                                 |
| ------------------- | ------------------------------------- |
| Architecture        | Decoder-Only Transformer (GPT-style)  |
| Parameters          | 10.8M (GPU) / 3.2M (CPU)              |
| Context Window      | 256 tokens                            |
| Embedding Dimension | 384 (GPU) / 256 (CPU)                 |
| Attention Heads     | 6 (GPU) / 4 (CPU)                     |
| Transformer Layers  | 6 (GPU) / 4 (CPU)                     |
| Optimizer           | AdamW (lr=3e-4)                       |
| Loss Function       | Cross-Entropy (next-token prediction) |
| Training Data       | Tiny Shakespeare (4.4M characters)    |
| Data Split          | 90/10 train/validation                |

## üî¨ Key Architectural Components

### 1. Scaled Dot-Product Multi-Head Attention

- Query, Key, Value projections for each head
- Scaling by 1/‚àöd_k prevents attention saturation
- 6 heads learn diverse patterns (syntax, semantics, discourse)

### 2. Causal Masking

- Triangular mask ensures auto-regressive property
- Tokens only attend to previous positions
- Critical for realistic next-token prediction

### 3. Residual Connections & Pre-LayerNorm

- Skip connections enable stable gradient flow in 6-layer network
- Pre-LN configuration improves training stability vs Post-LN
- Model learns to route information through depth

### 4. Feed-Forward Networks

- Position-wise FFN with 4x expansion in hidden layer
- ReLU non-linearity provides representational capacity
- Complements attention for sequence understanding

### 5. Learnable Positional Embeddings

- Adapted to task at hand (unlike fixed sinusoidal)
- Provides spatial awareness within sequences

## üìà Training Results

### Sample Quality Progression

- **Local (3.2M, 2K iters):** Character-level patterns, high noise
- **Colab (10.8M, 6K iters):** Character names, dialogue, punctuation
- **Kaggle (10.8M, 6K iters):** Consistent quality, proper formatting

### Convergence

```
Local:   4.60 ‚Üí 1.69 (2000 iters)
Colab:   4.59 ‚Üí 1.42 (6000 iters)  [GPU accelerated]
Kaggle:  4.59 ‚Üí 1.40 (6000 iters)  [GPU accelerated]
```

## üõ†Ô∏è Code Quality & Comments

All implementations include:

- **Architecture Comments:** "Why do we use softmax?" / "Why scale dot products?"
- **Data Handling:** Clear tokenization, batching, train/val split
- **Training Loop:** Progress tracking every 50-500 steps with ETA
- **Generation:** Auto-regressive sampling with temperature control (ready to extend)

See [gpt.py](gpt.py) for detailed inline explanations of each component.

## üìö Research Value

This project demonstrates:

1. **Foundational Knowledge:** Understanding Transformer internals, not just PyTorch APIs
2. **Scale-aware Design:** Adapting model size to available compute
3. **Empirical Validation:** Multiple independent training runs prove reproducibility
4. **Practical ML:** CPU prototyping ‚Üí GPU production workflow
5. **Language Learning:** Model discovers linguistic patterns from raw text

## üîó References

- "Attention Is All You Need" (Vaswani et al., 2017): https://arxiv.org/abs/1706.03762
- Tiny Shakespeare dataset: https://github.com/karpathy/char-rnn/data/tinyshakespeare
- PyTorch Documentation: https://pytorch.org/docs

## üìù License

Open source. Use for research and education.

---

**Status:** ‚úÖ Complete  
**Last Updated:** January 13, 2026  
**Training Platforms:** Local CPU, Google Colab, Kaggle Notebooks
